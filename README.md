# GruLM ğŸ§   
[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-orange)](https://pytorch.org/)

*A minimal GRU-based Language Model from scratch in PyTorch*

GruLM is a **lightweight language model** built using a stacked **GRU** architecture with optional **weight tying**, trained using **next-token prediction**.  
This project is intentionally simple and educational â€” designed to help you understand **language modeling fundamentals without transformers**.

---

## âœ¨ Features

- ğŸ” GRU-based causal language model
- ğŸ”— Optional **weight tying** (embedding â†” output projection)
- ğŸ§© SentencePiece tokenizer support
- ğŸ§ª Token-level accuracy + perplexity tracking
- ğŸ¯ Top-K + temperature sampling for inference
- ğŸ–¥ï¸ CPU-first, CUDA compatible

---

## ğŸ“‚ Project Structure

```text
.
â”œâ”€â”€ README.md
â”œâ”€â”€ dataloader.py # Tokenizer + Padded dataset loading 
â”œâ”€â”€ grulm.pt # output model
â”œâ”€â”€ inference.py # Text Generation Script
â”œâ”€â”€ model.py # GRU Language model 
â”œâ”€â”€ tokenizer.model # SentencePiece tokenizer model
â”œâ”€â”€ tokenizer.py # SenetencePiece custom tokenizer trainer
â””â”€â”€ train.py # Training Loop
```
---

ğŸ§  Model Architecture

Input IDs
   â†“
Embedding (V â†’ E)
   â†“
GRU (E â†’ H) Ã— N layers
   â†“
Linear (H â†’ V)
   â†“
Next-token logits

Key Details

Causal LM (predicts x[t+1] from x[:t])

Batch-first GRU

Weight tying supported if emb_dim == hdn_dim

No attention, no tricks â€” pure recurrent modeling



---

âš™ï¸ Requirements

pip install torch sentencepiece

Tested with:

Python â‰¥ 3.12


---

ğŸ‹ï¸ Training

The training loop uses teacher forcing with shifted inputs:

x = tokens[:, :-1]
y = tokens[:, 1:]

Run Training

python train.py

Training Highlights

CrossEntropyLoss with ignore_index = pad_id

Token-level accuracy (ignores padding)

Gradient clipping (clip_grad_norm_)

Perplexity reporting per epoch


Example output:

[Epoch 2] Sample 150/1000 | Loss 3.21 | TokenAcc 0.34
Avg NLL     : 3.05
Perplexity : 21.1


---

ğŸ—£ï¸ Inference / Text Generation

Supports:

Temperature scaling

Top-K sampling

Autoregressive decoding


Run Inference

python inference.py

Example:

Enter your prompt: hello world
Output:
<prompt> hello world <ai> this is a simple grulm demo ...

Sampling Logic

logits = logits / temperature
top_k filtering
softmax â†’ multinomial sampling


---

ğŸ™Œ Acknowledgements

PyTorch

SentencePiece

Classic RNN / LM literature

---
